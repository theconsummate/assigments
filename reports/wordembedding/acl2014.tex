%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Translation invariant Word Embeddings}

\author{First Author \\
  Affiliation / Address line 1 \\
  {\tt email@domain}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This report is a critical response to the paper \cite{huang2015translation} presented by me in the class. The report will first describe the contents of the paper briefly and then go into talking about the potential flaws and methodological errors which I have noticed. After that, I will do a comparison with \cite{faruqui2014improving} as the authors have used it as a baseline for their results.
\end{abstract}

\section{Introduction}
This paper tackles the problem of learning multi lingual word embeddings which are independent of the translations between the languages.  This concept is called “Translation Invariant word embeddings”.

Translation between languages is often represented by a probability distribution and therefore is a little ambiguous. For example, ``the" in english can translate to ``el" or ``a" in spanish with equal probability. The authors want to develop a framework which learns embeddings independently from the possible translations a word might have.

\section{Problem Definition}
The goal is to learn translation invariant embeddings with the following input data.
\begin{itemize}
\item A set of co-occurrence statistics between words in each of several languages. It is the frequency of a word occurring with another word in a particular context, either of the same language or another. 
\item A translation table containing alignment counts between words in each of these languages. For example, alignment matrices for “english-to-foreign” and “foreign-to-english”. These matrices represent a normalized probability distribution.
\end{itemize}

\section{Notation}

\subsection{X}
This represents a single multilingual co-occurrence matrix. The rows are the words and the columns are the contexts in which these words appear. The entries specify the co-occurrence count between a word in any language and a context in any language.

\begin{table}[h]  
\begin{center}
\begin{tabular}{ |c|c|c| } 
  \hline
  & cat & gato \\ 
  the & 2 & 0 \\ 
  el & 0 & 2 \\ 
  la & 0 & 0 \\ 
  \hline
\end{tabular}
\end{center}
\caption{\label{font-table} A sample matrix for X }
\end{table}

\subsection{D1}
A word alignment matrix with all the words (all languages) as both rows and columns.
Entries in this matrix specify which words are translations of which other words in a different language.


\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c|c| } 
  \hline
  & cat & gato \\ 
  cat & 0 & 1 \\ 
  gato & 1 & 0 \\ 
  \hline
\end{tabular}
\end{center}
\caption{\label{font-table} A sample matrix for D1 }
\end{table}

\subsection{D2}
A word alignment matrix with all the contexts (all languages) as both rows and columns.
Entries in this matrix specify which contexts are translations of which other contexts in a different language.


\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
  \hline
  & the & el & la \\ 
  the & 0 & 0.5 & 0.5 \\ 
  el & 1 & 0 & 0 \\ 
  la & 1 & 0 & 0 \\ 
  \hline
\end{tabular}
\end{center}
\caption{\label{font-table} A sample matrix for D2 }
\end{table}

\subsection{Find}
The goal is to find a latent representation for each word in each language that:
\begin{itemize}
  \item Captures information from X.
  \item Utilizes D1 and D2 to make the word representation “translation invariant”.
\end{itemize}


\section{Translation invariant LSA (TI-LSA)} \label{tilsa}
The classic method makes use of the equation: \begin{multline}\min_{U,V}\left\lVert X - UV^T \right\rVert^{2}_F\end{multline}

In this equation, the rows of U (or rows of V) represent the word embeddings. The solution of the equation is given by principal components of the singular value decomposition (SVD) of X.

Incorporating D1 and D2 and trying to simultaneously explain X and the various translation of it, the previous objective function can be written as:

\begin{multline}
\min_{U,V}\left\lVert X - UV^T \right\rVert^{2}_F + \left\lVert D_{1}X - UV^T \right\rVert^{2}_F \\+ \left\lVert XD^{T}_2 - UV^T \right\rVert^{2}_F + \left\lVert D_{1}XD^{T}_2 - UV^T \right\rVert^{2}_F
\end{multline}

This can be rewritten as \begin{multline}\min_{U,V}\left\lVert \widetilde{X} - UV^T \right\rVert^{2}_F\end{multline} where,
\begin{multline}
\widetilde{X} = \frac{1}{4}(X + D_{1}X + XD^{T}_2 +D_{1}XD^{T}_2 ) \\
= \frac{1}{4}(I + D_{1})X(I + D_2)^{T}
\end{multline}

\subsection{Lanczos Algorithm}
The new matrix $\widetilde{X}$ obtained above is not very sparse which means that calculation of SVD will not be efficient. To solve this problem, the paper uses the Lanczos algorithm \cite{van1996matrix}, which implements the calculation of SVD directly based on the factors without the need for carrying out multiplication explicitly. This method has linear complexity in the number of non-zeros present in the sparse matrices involved.
Thus, the time required for calculating the SVD of $\widetilde{X}$ is not much more than that of X.

\section{Experiments}
The paper compares its results with \cite{faruqui2014improving} and makes use of the same training data.
\subsection{Cross- lingual evaluation}
To compare with other techniques, a dependency parsing model of \cite{guo2015cross} was trained and the LAS and UAS scores were compared. The TI-LSA embeddings produces scores which were slightly better than \cite{faruqui2014improving} and \cite{guo2015cross}.

\subsection{Monolingual evaluation}
The embeddings were evaluated on monolingual word similarity tasks of \cite{faruqui2014community}. The average correlation was equal to vanilla LSA and only slightly worse than \cite{faruqui2014improving}. Thus this method does not deteriorate monolingual performance at the expense of crosslingual performance.

\subsection{Scalability}
Since the method is linear in the number of non-zeros in the data, the method can be scaled quite easily to many languages. The paper presents results for up to 4 languages and the run time increases linearly.

% Write a critical response to your paper. Highlight flaws, methodological errors,
% and/or shortcomings. Propose ways that the authors could have done things
% better, and highlight other works which avoid your criticisms of your paper.

\section{Methodological Errors}
\subsection{Missing description of data} \label{missingdata}
The paper states that it uses the same data as in \cite{faruqui2014improving} but does not give any details about what they do with it. Steps like preprocessing, removing bad data, split proportions between train, development and test sets etc are not at all talked about. It is quite possible that they are not doing any preprocessing and just plugging the whole data straight into their model but this fact should have been mentioned. \cite{faruqui2014improving} on the other hand talks about the data they used and provide information about the preprocessing steps.

\subsection{Lack of details about evaluation results}
For Monolingual evaluation, the paper presents an average of the correlation scores across 11 tasks described in \cite{faruqui2014community}. There is no description about what these tasks are and what they are trying to measure or evaluate. The paper also does not provide the scores for each of these tasks and how it compared with other word embedding techniques. The average score reported in the paper is not enough information for the readers to understand the finer details of this technique, where it excels and where it fails for monolingual tasks.

\subsection{Lack of implementation details}
The method described computes SVD of a very big matrix. It can be safely assumed that in the future, the size of training data is only going to get bigger and therefore efficient memory implementation is also required along with speed. There is no mention about how such large matrices were handled during implementation and what are the memory requirements. There is also no mention about the memory requirement of Lanczos Algorithm.

\section{Potential Flaws}
\subsection{Construction of D1 and D2 matrices}
\cite{faruqui2014improving} provide details about how they constructed their version of the X matrix (which is quite similar to this paper) but since they do not use D1 and D2, and as this paper also completely skips all details about the data as seen in \ref{missingdata}, it is quite unclear how these two matrices are obtained. The transformation process required to convert alignment matrices into D1 and D2 is very critical because these matrices contain the translation information and any error in their construction will have a huge ripple effect on the quality of the resulting embedding.

\subsection{Construction of the objective function}
\ref{tilsa} describes the process used in the paper to derive the objective function. The matrices D1 and D2 are added to the basic LSA equation by adding three more expressions. It is interesting to note that while the logic is sound, the effects of the individual terms is not studied. Like in Linear Regression, different predicting variables might explain different levels of variance, the different expressions involving D1 and D2 might contribute differently here, i.e. one expression might explain more variance than others.

In my view, the authors should have presented results for different objective functions and compared the results with each other to verify how much the additional information is contributing to the improvement of word vectors. Some obvious variations can be:

Only D1:
\begin{multline}
\min_{U,V}\left\lVert X - UV^T \right\rVert^{2}_F + \left\lVert D_{1}X - UV^T \right\rVert^{2}_F
\end{multline}

Only D2:
\begin{multline}
\min_{U,V}\left\lVert X - UV^T \right\rVert^{2}_F + \left\lVert XD^{T}_2 - UV^T \right\rVert^{2}_F
\end{multline}


\subsection{Overestimating the impact of Scalability}
The paper highlights the linear runtime of the method a lot and it is infact listed as one of the main advantage. It must be noted that most methods for obtaining word embeddings are infact linear in their runtime so it's not really that big of an advantage. Secondly, all the matrices are composed by concatenating the co-occurrence statistics for all the available languages. While this may be simple in theory, in general, it is very difficult to do in practice. For example, assuming a dataset of four languages, it will be very difficult to get pairwise translation probability for all the possible pairs to fill the matrices. Except popular languages like English, Mandarin etc, there is simply not so much pairwise data available for other languages publicly.

\subsection{Unaccounted impact of hyperparameters in dependency parsing}
The paper presents a comparison with \cite{guo2015cross} but it is also mentioned that the results could not be exactly reproduced due to difference in hyperparameters. It is also interesting to note that the difference between the average UAS score for the projection based heuristic of \cite{guo2015cross} and CCA as reported by them  is 1.66. This is in contrast to the difference reported in this paper, which is 0.6. This clearly shows that the effect of hyperparameters is quite prominent and thus the results should be taken with a grain of salt because it is quite possible that under a different hyperparameter configuration the TI-LSA embeddings might not outperform the other methods.

\section{Comparison with \cite{faruqui2014improving}}
\subsection{Similarities}
Both the papers build up on the distributional hypothesis which only considers the context of a word by extending to include "translational context" as well to improve word embeddings. What this means is that for any given word, the context includes the context in the source language (to which the word belongs) and the context of the corresponding translated word in a target language. It is important to note that the target languages can be multiple, depending upon the number of languages being considered in the task.


\subsection{Differences}
% provide analysis also
The method of incorporating "translational context" is the main difference between the two papers. \cite{faruqui2014improving} starts with word vectors and uses Canonical correlation analysis (CCA) to map two or more languages into a common shared space. The word vectors for each of the languages are transformed into a common space by learning a transformation matrix which optimizes the correlation between the two language vectors. Thus the goal is to make embeddings as similar as possible.

On the other hand, \cite{huang2015translation} starts with raw language data and learns the word vectors by incorporating the translational context into the learning algorithm. Therefore, the result is that different languages have their own vector space which has been enriched by extra information from a foreign language rather than having a shared vector space.


\section{Conclusion}
This paper presents a technique to incorporate translation data into the word embeddings using a modified LSA process. The technique gives superior performance on multilingual tasks and this ability does does not hurt performance on monolingual tasks. Finally, it scales linearly in the size of input data.


% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{acl2014}

\end{document}
