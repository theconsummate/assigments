%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Translation invariant Word Embeddings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This report describes \cite{huang2015translation}
\end{abstract}

\section{Introduction}
This paper tackles the problem of learning multi lingual word embeddings which are independent of the translations between the languages.  This concept is called “Translation Invariant word embeddings”.

Translation between languages is often represented by a probability distribution and therefore is a little ambiguous. For example, “the” in english can translate to “el” or “la” in spanish with equal probability. The authors want to develop a framework which learns embeddings independently from the possible translations a word might have.



\section{Problem Definition}
The goal is to learn translation invariant embeddings with the following input data.

- A set of co-occurrence statistics between words in each of several languages. It is the frequency of a word occurring with another word in a particular context, either of the same language or another. 
- A translation table containing alignment counts between words in each of these languages. For example, alignment matrices for “english-to-foreign” and “foreign-to-english”. These matrices represent a normalized probability distribution.

\section{Notation}

\subsection{X}
A single multilingual co-occurrence matrix.
Words as the rows and contexts as columns.
Entries specify the co-occurrence count between a word in any language and a context in any language.

\begin{center}
\begin{tabular}{ |c|c|c| } 
  \hline
  & cat & gato \\ 
  the & 2 & 0 \\ 
  el & 0 & 2 \\ 
  la & 0 & 0 \\ 
  \hline
\end{tabular}
\end{center}

\subsection{D1}
A word alignment matrix with all the words (all languages) as both rows and columns.
Entries in this matrix specify which words are translations of which other words.


\begin{center}
\begin{tabular}{ |c|c|c| } 
  \hline
  & cat & gato \\ 
  cat & 0 & 1 \\ 
  gato & 1 & 0 \\ 
  \hline
\end{tabular}
\end{center}

\subsection{D2}
A word alignment matrix with all the words (all languages) as both rows and columns.
Entries in this matrix specify which words are translations of which other words.


\begin{center}
\begin{tabular}{ |c|c|c|c| } 
  \hline
  & the & el & la \\ 
  the & 0 & 0.5 & 0.5 \\ 
  el & 1 & 0 & 0 \\ 
  la & 1 & 0 & 0 \\ 
  \hline
\end{tabular}
\end{center}

\subsection{Find}
A latent representation for each word in each language that :
Captures information from X.
Utilizes D1 and D2 to make the representation “translation invariant”.



\section{Translation invariant LSA}
The classic method makes use of the equation: <eq1>.

The rows of U (or rows of V) are the word embeddings.
The solution is given by principal components of the singular value decomposition (SVD) of X.

Incorporating D1 and D2 and trying to simultaneously explain X and the various translation of it, the previous objective function can be written as: <eq2>.

\subsection{Lanczos Algorithm}
The new matrix Ẋ is not very sparse.
Lanczos algorithm (Golub and Van Loan, 1996, Chapter 9), implements the calculation of SVD through the factors without the need for carrying out multiplication explicitly.
The method has linear complexity in the number of non-zeros present in the sparse matrices involved.
Thus, the time required for calculating the SVD of Ẋ is not much more than that of X.

\section{Experiments}
Three experiments presented:
Cross- lingual evaluation
Monolingual evaluation
Scalability



% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{acl2014}

\end{document}
